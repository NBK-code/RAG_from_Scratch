{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOK6TDnModf+AqfDGMK0I4i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NBK-code/RAG_from_Scratch/blob/main/RAG_with_Vector_DB_Reranker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieval-Augmented Generation"
      ],
      "metadata": {
        "id": "mjVh_6jRY4sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will improve upon our previous RAG implementation by adding the following features:\n",
        "\n",
        "1. Modular coding\n",
        "2. Clean Pipeline\n",
        "3. FAISS Vector Database\n",
        "4. Reranking Module\n",
        "5. Evaluation of RAG performance"
      ],
      "metadata": {
        "id": "pRbRA16e56XX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Install Necessary Software"
      ],
      "metadata": {
        "id": "wBU1udCS6aoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n",
        "!pip install spacy\n",
        "!pip install -U sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install bitsandbytes accelerate\n",
        "!pip install transformers\n",
        "!pip install flash-attn"
      ],
      "metadata": {
        "id": "MRQYS68ohPE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Project Folder and Files"
      ],
      "metadata": {
        "id": "YkUFV6bi6ir5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p rag_project"
      ],
      "metadata": {
        "id": "q1IF6rOfZAJZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "    \"ingestion.py\",\n",
        "    \"chunking.py\",\n",
        "    \"embedder.py\",\n",
        "    \"vector_db.py\",\n",
        "    \"rerank.py\",\n",
        "    \"retrieval.py\",\n",
        "    \"prompt_builder.py\",\n",
        "    \"llm.py\",\n",
        "    \"rag_engine.py\",\n",
        "    \"evaluate.py\",\n",
        "]\n",
        "\n",
        "for f in files:\n",
        "    open(f\"rag_project/{f}\", \"w\").close()\n",
        "\n",
        "print(\"Folder structure created!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-oQd4NOZEH9",
        "outputId": "8793a481-fa0f-4053-9819-04d5f251e972"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder structure created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create All The Modules"
      ],
      "metadata": {
        "id": "d3_xZyTY6r6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Ingestion"
      ],
      "metadata": {
        "id": "CFkUfviW6xKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_project/ingestion.py\n",
        "\"\"\"\n",
        "ingestion.py\n",
        "\n",
        "Lightweight ingestion module based on the original rag_from_scratch.py.\n",
        "We keep your logic, but modularize it so the rest of the pipeline can use it cleanly.\n",
        "\"\"\"\n",
        "\n",
        "import fitz\n",
        "\n",
        "def text_formatter(text: str) -> str:\n",
        "    \"\"\"Formats text by removing newlines and collapsing spaces.\"\"\"\n",
        "    return text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "def open_and_read_pdf(pdf_path: str):\n",
        "    \"\"\"\n",
        "    Open and read a PDF file.\n",
        "    Returns a list of dictionaries, one per page.\n",
        "    Keeps your original metadata structure.\n",
        "    \"\"\"\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "    pdf_pages_and_texts = []\n",
        "\n",
        "    for page_number, page in enumerate(pdf_document):\n",
        "        text = page.get_text()\n",
        "        text = text_formatter(text)\n",
        "\n",
        "        pdf_pages_and_texts.append({\n",
        "            \"page_number\": page_number,\n",
        "            \"page_char_count\": len(text),\n",
        "            \"page_word_count\": len(text.split(\" \")),\n",
        "            \"page_sentence_count_raw\": len(text.split(\". \")),\n",
        "            \"page_token_count_raw\": len(text) / 4,   # approx. 4 chars = 1 token\n",
        "            \"text\": text,\n",
        "        })\n",
        "\n",
        "    return pdf_pages_and_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-Ky2hICZOVE",
        "outputId": "c3bf71f3-e48b-4bb7-9a09-8c68c5eba0ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_project/ingestion.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chunking"
      ],
      "metadata": {
        "id": "cOp5r3tI66BX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunks each page of the pdf document."
      ],
      "metadata": {
        "id": "pTFoNF_i7A8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_project/chunking.py\n",
        "\"\"\"\n",
        "chunking.py\n",
        "\n",
        "Clean, modular version of your original chunking logic.\n",
        "- Uses spaCy sentencizer\n",
        "- Joins sentences\n",
        "- Fixes punctuation spacing issues\n",
        "- Produces chunks of N sentences\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "\n",
        "def load_spacy():\n",
        "    \"\"\"Load spaCy sentencizer only once.\"\"\"\n",
        "    nlp = spacy.blank(\"en\")\n",
        "    nlp.add_pipe(\"sentencizer\")\n",
        "    return nlp\n",
        "\n",
        "\n",
        "def fix_spacing(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Fix cases like:\n",
        "        'How are you?I am fine' --> 'How are you? I am fine'\n",
        "    Based on your original requirement.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'\\.([A-Z])', r'. \\1', text)\n",
        "    text = re.sub(r'\\?([A-Z])', r'? \\1', text)\n",
        "    text = re.sub(r'\\!([A-Z])', r'! \\1', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def split_into_sentences(nlp, text: str):\n",
        "    \"\"\"\n",
        "    Split a paragraph into sentences using spaCy sentencizer.\n",
        "    Returns a list of sentence strings.\n",
        "    \"\"\"\n",
        "    text = fix_spacing(text)\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def chunk_sentences(sentences, chunk_size=10):\n",
        "    \"\"\"\n",
        "    Group sentences into chunks of fixed size.\n",
        "    Returns a list of strings (chunks).\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        current_chunk.append(sent)\n",
        "\n",
        "        if len(current_chunk) >= chunk_size:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "\n",
        "    # Add leftover sentences as final chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def chunk_page_text(nlp, page_dict, chunk_size=10):\n",
        "    \"\"\"\n",
        "    Convert a page dict (from ingestion module) to a list of chunk dicts.\n",
        "    Each chunk has:\n",
        "        - text\n",
        "        - metadata (page number, chunk index)\n",
        "    \"\"\"\n",
        "    sentences = split_into_sentences(nlp, page_dict[\"text\"])\n",
        "    chunks = chunk_sentences(sentences, chunk_size)\n",
        "\n",
        "    chunk_dicts = []\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        chunk_dicts.append({\n",
        "            \"page_number\": page_dict[\"page_number\"],\n",
        "            \"chunk_id\": idx,\n",
        "            \"text\": chunk,\n",
        "        })\n",
        "\n",
        "    return chunk_dicts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TphPR16ghr7v",
        "outputId": "5b5bbddf-98e2-42b0-f63e-987746803b01"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_project/chunking.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Embed Text and Query"
      ],
      "metadata": {
        "id": "FcRsoFJ-8HP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_project/embedder.py\n",
        "\"\"\"\n",
        "embedder.py\n",
        "\n",
        "Unified embedding module for:\n",
        "- Text chunk embeddings (for FAISS)\n",
        "- Query embeddings (for retrieval)\n",
        "\n",
        "Uses: BAAI/bge-base-en-v1.5\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class Embedder:\n",
        "    def __init__(self, model_name=\"BAAI/bge-base-en-v1.5\", device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        Initialize the embedding model.\n",
        "        device can be 'cpu' or 'cuda'\n",
        "        \"\"\"\n",
        "        print(f\"[Embedder] Loading model: {model_name} on {device}\")\n",
        "        self.model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "    def embed_texts(self, texts, batch_size=16):\n",
        "        \"\"\"\n",
        "        Embed a list of texts (chunks).\n",
        "        Returns numpy array of shape (N, dim).\n",
        "        \"\"\"\n",
        "        embeddings = self.model.encode(\n",
        "            texts,\n",
        "            batch_size=batch_size,\n",
        "            show_progress_bar=True,\n",
        "            convert_to_numpy=True,\n",
        "            normalize_embeddings=True,\n",
        "        )\n",
        "        return embeddings.astype(np.float32)\n",
        "\n",
        "    def embed_query(self, query):\n",
        "        \"\"\"\n",
        "        Embed a single query string.\n",
        "        Returns 1 vector (dim,).\n",
        "        \"\"\"\n",
        "        vec = self.model.encode(\n",
        "            query,\n",
        "            normalize_embeddings=True,\n",
        "            convert_to_numpy=True,\n",
        "        )\n",
        "        return vec.astype(np.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad_vZ8n9s4mn",
        "outputId": "c622c46c-d0a0-4be4-fa54-ef5cd48f87ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_project/embedder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Vector Database"
      ],
      "metadata": {
        "id": "455AfN9N8Rkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vector database is a special kind of database that stores vectors (numerical embeddings) instead of text.\n",
        "\n",
        "FAISS stands for Facebook AI Similarity Search.\n",
        "\n",
        "It is not a full database server ‚Äî it is a very fast library that:\n",
        "\n",
        "1. Stores vectors in memory\n",
        "2. Performs similarity search extremely fast\n",
        "3. Supports GPU acceleration\n",
        "4. Widely used in RAG systems\n",
        "\n",
        "Here we create a vector DB locally - all the vectors are stored in colab's file system.\n",
        "\n",
        "Common FAISS Index Types\n",
        "\n",
        "| **Index Type**      | **Exact?** | **Best Dataset Size** | **Speed**        | **Memory**     | **When to Use** |\n",
        "|---------------------|-----------|------------------------|------------------|----------------|------------------|\n",
        "| **Flat**            | ‚úÖ Exact  | 1k‚Äì50k                 | Slowest          | High           | Exact L2 distance; rarely needed in RAG |\n",
        "| **FlatIP**          | ‚úÖ Exact  | 1k‚Äì100k                | Slow (but fast in FAISS) | High | **Best for small/medium RAG**; exact cosine similarity (normalized embeddings) |\n",
        "| **HNSW32**          | ‚ö†Ô∏è Approx | 50k‚Äì10M                | Very Fast        | Medium         | **Most popular ANN**; great recall + speed balance |\n",
        "| **IVF100**          | ‚ö†Ô∏è Approx | 100k‚Äì20M               | Very Fast        | Medium         | Cluster-based search; standard for large datasets |\n",
        "| **IVF100,PQ16**     | ‚ö†Ô∏è Approx | 500k‚Äì100M              | Extremely Fast   | Very Low       | IVF + Product Quantization; memory-efficient for huge corpora |\n"
      ],
      "metadata": {
        "id": "LiBJ5PDU-Ji2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_project/vector_db.py\n",
        "\"\"\"\n",
        "vector_db.py\n",
        "\n",
        "FAISS-based vector database for text RAG.\n",
        "Uses IndexFlatIP (exact inner product search).\n",
        "\"\"\"\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class FAISSVectorDB:\n",
        "    def __init__(self, dim):\n",
        "        \"\"\"\n",
        "        Initialize a Flat (exact) inner-product FAISS index.\n",
        "        Works perfectly for normalized BGE embeddings.\n",
        "        \"\"\"\n",
        "        self.index = faiss.IndexFlatIP(dim)  # exact cosine similarity\n",
        "        self.metadata_store = []\n",
        "        self.dim = dim\n",
        "\n",
        "        print(f\"[FAISS] Created IndexFlatIP with dim={dim}\")\n",
        "\n",
        "    def add_embeddings(self, embeddings, metadata_list):\n",
        "        embeddings = embeddings.astype(np.float32)\n",
        "\n",
        "        if len(embeddings) != len(metadata_list):\n",
        "            raise ValueError(\"embeddings and metadata_list must have same length\")\n",
        "\n",
        "        self.index.add(embeddings)\n",
        "        self.metadata_store.extend(metadata_list)\n",
        "\n",
        "        print(f\"[FAISS] Added {len(embeddings)} vectors\")\n",
        "\n",
        "    def search(self, query_vector, top_k=5):\n",
        "        query_vector = query_vector.reshape(1, -1).astype(np.float32)\n",
        "        scores, indices = self.index.search(query_vector, top_k)\n",
        "\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx == -1:\n",
        "                continue\n",
        "            results.append({\n",
        "                \"score\": float(score),\n",
        "                \"metadata\": self.metadata_store[idx]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save(self, index_path, metadata_path):\n",
        "        faiss.write_index(self.index, index_path)\n",
        "        np.save(metadata_path, self.metadata_store, allow_pickle=True)\n",
        "        print(f\"[FAISS] Saved index to {index_path}\")\n",
        "        print(f\"[FAISS] Saved metadata to {metadata_path}\")\n",
        "\n",
        "    def load(self, index_path, metadata_path):\n",
        "        self.index = faiss.read_index(index_path)\n",
        "        self.metadata_store = np.load(metadata_path, allow_pickle=True).tolist()\n",
        "        print(f\"[FAISS] Loaded index from {index_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rau_UnD4FWg",
        "outputId": "b16c8043-5049-4bdb-f132-29ef61b81dc1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_project/vector_db.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Reranker"
      ],
      "metadata": {
        "id": "5fO8jzFuTdjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding search gives us only a coarse matches. For improved performance, we use a reranker.\n",
        "\n",
        "####Cross-Encoder Reranker\n",
        "\n",
        "- **Architecture:** A single transformer processes *query + passage together* (`[CLS] query [SEP] passage [SEP]`) with full cross-attention. Query tokens attend to passage tokens and vice-versa, enabling deep pairwise comparison. Outputs a score (relevance from 0 ‚Üí 1 or ‚àíinf ‚Üí +inf).\n",
        "- **Training data:** Labeled *(query, positive passage)* and *(query, negative passage)* pairs.  \n",
        "- **Training objective:** Ranking-focused losses (binary cross-entropy, contrastive, softmax cross-entropy) that push positive passages to score higher than negatives.  \n",
        "- **Purpose in RAG:** FAISS retrieves broadly relevant chunks; the reranker selects the *most* relevant ones, improving precision and answer quality significantly.\n"
      ],
      "metadata": {
        "id": "GGrsMhiSUEBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_project/rerank.py\n",
        "\"\"\"\n",
        "rerank.py\n",
        "\n",
        "Cross-encoder reranking to improve retrieval quality.\n",
        "Uses BAAI/bge-reranker-base.\n",
        "\"\"\"\n",
        "\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "\n",
        "class Reranker:\n",
        "    def __init__(self, model_name=\"BAAI/bge-reranker-base\", device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        Initialize the cross-encoder reranker.\n",
        "        \"\"\"\n",
        "        print(f\"[Reranker] Loading model: {model_name} on {device}\")\n",
        "        self.model = CrossEncoder(model_name, device=device)\n",
        "\n",
        "    def rerank(self, query, candidate_chunks, top_k=5):\n",
        "        \"\"\"\n",
        "        Rerank candidate chunks based on relevance to the query.\n",
        "\n",
        "        query: string\n",
        "        candidate_chunks: list of dicts, each containing {metadata, score, text}\n",
        "                          usually returned by FAISS\n",
        "\n",
        "        Returns the top_k reranked chunks with 'rerank_score'.\n",
        "        \"\"\"\n",
        "\n",
        "        # Prepare input pairs for cross-encoder\n",
        "        pairs = []\n",
        "        for item in candidate_chunks:\n",
        "            text = item[\"metadata\"][\"text\"]\n",
        "            pairs.append([query, text])\n",
        "\n",
        "        # Compute cross-encoder scores\n",
        "        scores = self.model.predict(pairs)\n",
        "\n",
        "        # Attach scores to chunks\n",
        "        for i, item in enumerate(candidate_chunks):\n",
        "            item[\"rerank_score\"] = float(scores[i])\n",
        "\n",
        "        # Sort by rerank score (descending)\n",
        "        reranked = sorted(candidate_chunks, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "        return reranked[:top_k]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARFTkL7iTnt1",
        "outputId": "9fc672b0-ab53-4324-d761-d7e3bb6f9e1d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_project/rerank.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Retrieval"
      ],
      "metadata": {
        "id": "ZHKGMR7vvZdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_project/retrieval.py\n",
        "\"\"\"\n",
        "retrieval.py\n",
        "\n",
        "Unified retrieval module:\n",
        "- Embeds the query\n",
        "- Searches FAISS vector DB\n",
        "- Optionally applies Cross-Encoder reranking\n",
        "\"\"\"\n",
        "\n",
        "from embedder import Embedder\n",
        "from rerank import Reranker\n",
        "from vector_db import FAISSVectorDB\n",
        "\n",
        "\n",
        "class Retriever:\n",
        "    def __init__(self, embedder: Embedder, vector_db: FAISSVectorDB,\n",
        "                 reranker: Reranker = None,\n",
        "                 initial_k: int = 20,\n",
        "                 final_k: int = 5):\n",
        "        \"\"\"\n",
        "        embedder: Embedder object\n",
        "        vector_db: FAISSVectorDB object\n",
        "        reranker: Reranker object (optional)\n",
        "        initial_k: how many candidates to pull from FAISS\n",
        "        final_k: how many results to return after reranking\n",
        "        \"\"\"\n",
        "        self.embedder = embedder\n",
        "        self.vector_db = vector_db\n",
        "        self.reranker = reranker\n",
        "        self.initial_k = initial_k\n",
        "        self.final_k = final_k\n",
        "\n",
        "        if reranker:\n",
        "            print(\"[Retriever] Reranking enabled.\")\n",
        "        else:\n",
        "            print(\"[Retriever] Reranking disabled.\")\n",
        "\n",
        "    def retrieve(self, query):\n",
        "        \"\"\"\n",
        "        Retrieve top chunks for the given query.\n",
        "        Uses FAISS search, then optional reranking.\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Embed query\n",
        "        query_vec = self.embedder.embed_query(query)\n",
        "\n",
        "        # 2. Search FAISS\n",
        "        results = self.vector_db.search(query_vec, top_k=self.initial_k)\n",
        "\n",
        "        # 3. If reranker is OFF ‚Üí return FAISS results only\n",
        "        if self.reranker is None:\n",
        "            return results[:self.final_k]\n",
        "\n",
        "        # 4. If reranker is ON ‚Üí rerank the FAISS candidates\n",
        "        reranked = self.reranker.rerank(query, results, top_k=self.final_k)\n",
        "\n",
        "        return reranked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49oncT87vTaP",
        "outputId": "fe57ce38-c1ab-47fa-8efa-f17d6310f0f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_project/retrieval.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Augmentation - Prompt Builder"
      ],
      "metadata": {
        "id": "MkO0r1071f_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_project/prompt_builder.py\n",
        "\"\"\"\n",
        "prompt_builder.py\n",
        "\n",
        "Builds a clean prompt for LLM inference using retrieved chunks.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class PromptBuilder:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def build_prompt(self, query, retrieved_chunks):\n",
        "        \"\"\"\n",
        "        query: string\n",
        "        retrieved_chunks: list of dicts (from retriever)\n",
        "                          each dict has {score, metadata: {text, ...}}\n",
        "\n",
        "        Returns a final LLM prompt string.\n",
        "        \"\"\"\n",
        "\n",
        "        # Extract chunk texts\n",
        "        context_lines = []\n",
        "        for i, item in enumerate(retrieved_chunks, start=1):\n",
        "            text = item[\"metadata\"][\"text\"]\n",
        "            context_lines.append(f\"[{i}] {text}\")\n",
        "\n",
        "        context_block = \"\\n\".join(context_lines)\n",
        "\n",
        "        # Construct final prompt\n",
        "        prompt = f\"\"\"\n",
        "You are a helpful assistant. Use ONLY the information provided in the context below.\n",
        "If the answer is not contained in the context, respond with \"I don't know.\"\n",
        "\n",
        "### Question:\n",
        "{query}\n",
        "\n",
        "### Context:\n",
        "{context_block}\n",
        "\n",
        "### Answer:\"\"\".strip()\n",
        "\n",
        "        return prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewfgcrzu1pDh",
        "outputId": "8f48fed0-ffc0-488a-a5cd-879a22adf462"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_project/prompt_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LLM"
      ],
      "metadata": {
        "id": "CNa22A3y61_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_project/llm.py\n",
        "\"\"\"\n",
        "llm.py\n",
        "\n",
        "Loads Gemma 2B or Gemma 7B based on GPU memory.\n",
        "Supports optional 4-bit quantization.\n",
        "Provides a simple .generate(prompt) interface.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.utils import is_flash_attn_2_available\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "\n",
        "class LLM:\n",
        "    def __init__(self):\n",
        "        print(\"[LLM] Detecting GPU memory...\")\n",
        "        gpu_memory_bytes = torch.cuda.get_device_properties(0).total_memory\n",
        "        gpu_memory_in_gb = round(gpu_memory_bytes / (2**30))\n",
        "        print(f\"[LLM] Available GPU memory: {gpu_memory_in_gb} GB\")\n",
        "\n",
        "        # Choose model\n",
        "        if gpu_memory_in_gb < 5.1:\n",
        "            print(f\"[LLM] {gpu_memory_in_gb}GB ‚Äî Gemma 7B is too large.\")\n",
        "            print(\"[LLM] You should use Gemma 2B with 4-bit quantization.\")\n",
        "            self.use_quant = True\n",
        "            model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "        elif gpu_memory_in_gb < 8.1:\n",
        "            print(f\"[LLM] {gpu_memory_in_gb}GB ‚Äî Recommended: Gemma 2B in 4-bit.\")\n",
        "            self.use_quant = True\n",
        "            model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "        elif gpu_memory_in_gb < 19.0:\n",
        "            print(f\"[LLM] {gpu_memory_in_gb}GB ‚Äî Gemma 2B fp16 or Gemma 7B in 4-bit.\")\n",
        "            self.use_quant = False\n",
        "            model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "        else:\n",
        "            print(f\"[LLM] {gpu_memory_in_gb}GB ‚Äî Gemma 7B (fp16 or 4-bit).\")\n",
        "            self.use_quant = False\n",
        "            model_id = \"google/gemma-7b-it\"\n",
        "\n",
        "        print(f\"[LLM] use_quantization = {self.use_quant}\")\n",
        "        print(f\"[LLM] Loading model: {model_id}\\n\")\n",
        "\n",
        "        # Flash Attention 2 support\n",
        "        if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
        "            attn_impl = \"flash_attention_2\"\n",
        "            print(\"[LLM] Using Flash Attention 2\")\n",
        "        else:\n",
        "            attn_impl = \"sdpa\"\n",
        "            print(\"[LLM] Using Scaled Dot-Product Attention\")\n",
        "\n",
        "        # Tokenizer\n",
        "        print(\"[LLM] Loading tokenizer...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "        # Optional 4-bit quantization\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        ) if self.use_quant else None\n",
        "\n",
        "        # Model\n",
        "        print(\"[LLM] Loading model weights (this may take time)...\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            pretrained_model_name_or_path=model_id,\n",
        "            torch_dtype=torch.float16,\n",
        "            quantization_config=quant_config,\n",
        "            low_cpu_mem_usage=False,\n",
        "            attn_implementation=attn_impl\n",
        "        )\n",
        "\n",
        "        # If not quantized ‚Üí move to GPU manually\n",
        "        if not self.use_quant:\n",
        "            self.model.to(\"cuda\")\n",
        "\n",
        "        print(\"[LLM] Model is ready.\\n\")\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens=256, temperature=0.2):\n",
        "        \"\"\"\n",
        "        Generate text from the LLM using a clean prompt.\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        output_ids = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=False,  # deterministic for RAG\n",
        "        )\n",
        "\n",
        "        output_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Remove prompt prefix if repeated\n",
        "        if output_text.startswith(prompt):\n",
        "            output_text = output_text[len(prompt):].strip()\n",
        "\n",
        "        return output_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmL9y8dF59oh",
        "outputId": "547b87fc-3d38-484b-c035-d31ee0d916d1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_project/llm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Augmented Generation"
      ],
      "metadata": {
        "id": "t2zxJxPN64b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rag_project/rag_engine.py\n",
        "\"\"\"\n",
        "rag_engine.py\n",
        "\n",
        "The unified RAG pipeline:\n",
        "- Takes a user query\n",
        "- Retrieves relevant chunks (FAISS + optional reranking)\n",
        "- Builds a grounded prompt\n",
        "- Generates an answer using the LLM\n",
        "- Returns both answer and retrieved chunks (for debugging)\n",
        "\"\"\"\n",
        "\n",
        "from retrieval import Retriever\n",
        "from prompt_builder import PromptBuilder\n",
        "from llm import LLM\n",
        "\n",
        "\n",
        "class RAGEngine:\n",
        "    def __init__(self, retriever: Retriever, llm: LLM):\n",
        "        \"\"\"\n",
        "        retriever: Retriever object (FAISS + optional reranker)\n",
        "        llm: LLM object (Gemma 2B/7B wrapper)\n",
        "        \"\"\"\n",
        "        self.retriever = retriever\n",
        "        self.llm = llm\n",
        "        self.prompt_builder = PromptBuilder()\n",
        "\n",
        "        print(\"[RAGEngine] Initialized RAG engine\")\n",
        "\n",
        "    def answer(self, query: str, return_chunks: bool = False):\n",
        "        \"\"\"\n",
        "        Runs the full RAG pipeline:\n",
        "\n",
        "        1. Retrieve relevant chunks\n",
        "        2. Build prompt\n",
        "        3. Generate answer\n",
        "        4. Optionally return chunks for evaluation/debugging\n",
        "        \"\"\"\n",
        "        print(f\"\\n[RAGEngine] Query: {query}\")\n",
        "\n",
        "        # 1. Retrieve top chunks\n",
        "        print(\"[RAGEngine] Retrieving chunks...\")\n",
        "        retrieved_chunks = self.retriever.retrieve(query)\n",
        "\n",
        "        # 2. Build LLM prompt\n",
        "        print(\"[RAGEngine] Building prompt...\")\n",
        "        prompt = self.prompt_builder.build_prompt(query, retrieved_chunks)\n",
        "\n",
        "        # 3. Get LLM answer\n",
        "        print(\"[RAGEngine] Generating answer...\")\n",
        "        answer = self.llm.generate(prompt)\n",
        "\n",
        "        if return_chunks:\n",
        "            return {\n",
        "                \"answer\": answer,\n",
        "                \"chunks\": retrieved_chunks,\n",
        "                \"prompt\": prompt\n",
        "            }\n",
        "        else:\n",
        "            return answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8SkxeUC665N",
        "outputId": "c0d801b3-9ce0-485b-9c6a-4bfe0e7d1d09"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rag_project/rag_engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Orchestrate the Model"
      ],
      "metadata": {
        "id": "LS2JBoGcFtZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"Your HF access token here\")"
      ],
      "metadata": {
        "id": "fXt8AkQnF2pC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "astronomy_questions = [\n",
        "    \"What physical processes determine how long a star remains on the main sequence?\",\n",
        "    \"Why do massive stars evolve more quickly than low-mass stars despite having more fuel?\",\n",
        "    \"What roles do convection and radiation play inside stars, and how do you identify which zone dominates?\",\n",
        "    \"How do nuclear reaction rates influence the internal structure of a star?\",\n",
        "    \"What physical conditions lead a star to end its life as a white dwarf, neutron star, or black hole?\",\n",
        "    \"Why is the spectrum of a star not a perfect blackbody?\",\n",
        "    \"What determines the width and shape of spectral lines in stellar spectra?\",\n",
        "    \"In what situations does the assumption of local thermodynamic equilibrium break down in a stellar atmosphere?\",\n",
        "    \"How does the opacity of a star's atmosphere influence the emergent spectrum?\",\n",
        "    \"What information about a star can be inferred from its spectral classification?\",\n",
        "    \"What evidence in spiral galaxies suggests the presence of dark matter?\",\n",
        "    \"Why do elliptical galaxies contain little cold gas compared to spiral galaxies?\",\n",
        "    \"How do astronomers infer the orbital motions of stars inside a galaxy?\",\n",
        "    \"What physical processes influence the shape and structure of galaxies over time?\",\n",
        "    \"How do galaxy interactions and mergers affect galactic evolution?\",\n",
        "    \"What observations support the idea that the universe is expanding?\",\n",
        "    \"Why is the cosmic microwave background considered strong evidence for the early hot universe?\",\n",
        "    \"How do astronomers measure distances to very distant galaxies?\",\n",
        "    \"What distinguishes dark matter from dark energy in terms of their observable effects on the universe?\",\n",
        "    \"What role do galaxy clusters play in understanding large-scale structure?\",\n",
        "    \"What physical conditions lead to the formation of an accretion disk around a compact object?\",\n",
        "    \"Why are some black holes strong sources of X-rays?\",\n",
        "    \"How do astronomers determine whether an observed compact object is likely a neutron star or a black hole?\",\n",
        "    \"What processes can accelerate particles to relativistic speeds in astrophysical environments?\",\n",
        "    \"How do supernova explosions influence their surrounding interstellar medium?\",\n",
        "    \"How do transiting exoplanets produce measurable changes in starlight?\",\n",
        "    \"What factors determine whether a planet can retain an atmosphere?\",\n",
        "    \"How do planetary migration theories explain the presence of hot Jupiters?\",\n",
        "    \"Why are some planetary systems so different from our solar system?\",\n",
        "    \"What methods allow astronomers to study the atmospheres of exoplanets?\",\n",
        "    \"What factors limit the sensitivity of a ground-based telescope?\",\n",
        "    \"How do astronomers distinguish between signal and noise in an astronomical observation?\",\n",
        "    \"Why are space telescopes necessary for certain wavelengths of light?\",\n",
        "    \"What determines the resolving power of an astronomical instrument (qualitatively)?\",\n",
        "    \"How do different types of detectors differ in how they measure incoming light?\"\n",
        "]\n",
        "\n",
        "query_list = astronomy_questions\n",
        "\n",
        "\n",
        "!wget -O astronomy.pdf https://www.as.utexas.edu/~elr/Astronomy-LR.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIAtG2RWW81L",
        "outputId": "c7ff9e80-0877-4d7b-9e7c-2e1ca0e49fa6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-06 14:44:56--  https://www.as.utexas.edu/~elr/Astronomy-LR.pdf\n",
            "Resolving www.as.utexas.edu (www.as.utexas.edu)... 128.83.20.6\n",
            "Connecting to www.as.utexas.edu (www.as.utexas.edu)|128.83.20.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41720608 (40M) [application/pdf]\n",
            "Saving to: ‚Äòastronomy.pdf‚Äô\n",
            "\n",
            "astronomy.pdf       100%[===================>]  39.79M  54.0MB/s    in 0.7s    \n",
            "\n",
            "2026-02-06 14:44:57 (54.0 MB/s) - ‚Äòastronomy.pdf‚Äô saved [41720608/41720608]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import random\n",
        "import torch\n",
        "\n",
        "sys.path.append(\"/content/rag_project\")"
      ],
      "metadata": {
        "id": "G2Utmqy_XG1e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ingestion import open_and_read_pdf\n",
        "from chunking import load_spacy, chunk_page_text\n",
        "from embedder import Embedder\n",
        "from vector_db import FAISSVectorDB\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load PDF\n",
        "print(f\"[INFO] Loading PDF...\")\n",
        "pages = open_and_read_pdf(\"astronomy.pdf\")\n",
        "\n",
        "# 2. Chunk all pages\n",
        "print(f\"[INFO] Chunking pages...\")\n",
        "nlp = load_spacy()\n",
        "all_chunks = []\n",
        "for p in pages:\n",
        "    all_chunks.extend(chunk_page_text(nlp, p))\n",
        "\n",
        "# 3. Embed all chunks\n",
        "print(f\"[INFO] Embedding chunks...\")\n",
        "emb = Embedder(device=\"cuda\")\n",
        "texts = [c[\"text\"] for c in all_chunks]\n",
        "embeddings = emb.embed_texts(texts, batch_size=32)\n",
        "\n",
        "# 4. Build FAISS DB\n",
        "print(f\"[INFO] Building FAISS DB...\")\n",
        "db = FAISSVectorDB(dim=embeddings.shape[1])\n",
        "db.add_embeddings(embeddings, all_chunks)\n",
        "\n",
        "# 5. Save\n",
        "print(f\"[INFO] Saving FAISS DB...\")\n",
        "db.save(\"faiss_index.faiss\", \"metadata.npy\")"
      ],
      "metadata": {
        "id": "Jwe-e1dYXVc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vector_db import FAISSVectorDB\n",
        "from embedder import Embedder\n",
        "from rerank import Reranker\n",
        "from retrieval import Retriever\n",
        "from prompt_builder import PromptBuilder\n",
        "from llm import LLM\n",
        "from rag_engine import RAGEngine\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load vector DB\n",
        "db = FAISSVectorDB(dim=768)\n",
        "db.load(\"faiss_index.faiss\", \"metadata.npy\")\n",
        "\n",
        "# 2. Load embedder + reranker + llm\n",
        "emb = Embedder(device=\"cuda\")\n",
        "reranker = Reranker(device=\"cuda\")\n",
        "retriever = Retriever(embedder=emb, vector_db=db, reranker=reranker)\n",
        "llm = LLM()\n",
        "\n",
        "# 3. Build RAG engine\n",
        "engine = RAGEngine(retriever, llm)"
      ],
      "metadata": {
        "id": "cYpXhH8SXWjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "while True:\n",
        "    q = input(\"\\nAsk a question (or type 'exit' to quit): \").strip()\n",
        "\n",
        "    if q.lower() in (\"exit\", \"quit\", \"\"):\n",
        "        print(\"\\nExiting RAG system. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    result = engine.answer(q, return_chunks=True)\n",
        "\n",
        "    answer = result[\"answer\"]\n",
        "    chunks = result[\"chunks\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üü¶ FINAL ANSWER\")\n",
        "    print(\"=\" * 60)\n",
        "    print(textwrap.fill(answer, width=80))   # wrap answer text\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üü© TOP RETRIEVED CHUNKS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    for i, chunk in enumerate(chunks, start=1):\n",
        "\n",
        "        # Pick rerank_score if available, otherwise FAISS score\n",
        "        score = chunk.get(\"rerank_score\", chunk.get(\"score\", None))\n",
        "        score_str = f\"{score:.4f}\" if score is not None else \"N/A\"\n",
        "\n",
        "        text_preview = chunk[\"metadata\"][\"text\"].strip()\n",
        "\n",
        "        # Optionally shorten preview (keep first 600 chars)\n",
        "        if len(text_preview) > 600:\n",
        "            text_preview = text_preview[:600] + \"...\"\n",
        "\n",
        "        print(f\"\\n[{i}]  Score = {score_str}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Wrap chunk text to avoid going off-screen\n",
        "        print(textwrap.fill(text_preview, width=80))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyiR0tEXdNie",
        "outputId": "7a8a35e0-388f-4859-e39e-ea5405710e1f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ask a question (or type 'exit' to quit): Why is Sun yellow in color?\n",
            "\n",
            "[RAGEngine] Query: Why is Sun yellow in color?\n",
            "[RAGEngine] Retrieving chunks...\n",
            "[RAGEngine] Building prompt...\n",
            "[RAGEngine] Generating answer...\n",
            "\n",
            "============================================================\n",
            "üü¶ FINAL ANSWER\n",
            "============================================================\n",
            "The Sun looks yellow in color as seen from Earth‚Äôs surface because the nitrogen\n",
            "molecules in our planet‚Äôs atmosphere scatter some of the shorter (i.e., blue)\n",
            "wavelengths of light out of the beams of sunlight that reach us, leaving more\n",
            "long wavelength light behind.\n",
            "\n",
            "============================================================\n",
            "üü© TOP RETRIEVED CHUNKS\n",
            "============================================================\n",
            "\n",
            "[1]  Score = 0.6960\n",
            "------------------------------------------------------------\n",
            "Example Star Colors and Corresponding Approximate Temperatures Star Color\n",
            "Approximate Temperature Example Orange 4000 K Aldebaran Red 3000 K Betelgeuse\n",
            "Table 17.1 The hottest stars have temperatures of over 40,000 K, and the coolest\n",
            "stars have temperatures of about 2000 K. Our Sun‚Äôs surface temperature is about\n",
            "6000 K; its peak wavelength color is a slightly greenish-yellow. In space, the\n",
            "Sun would look white, shining with about equal amounts of reddish and bluish\n",
            "wavelengths of light. It looks somewhat yellow as seen from Earth‚Äôs surface\n",
            "because our planet‚Äôs nitrogen molecules scatter some of...\n",
            "\n",
            "[2]  Score = 0.4820\n",
            "------------------------------------------------------------\n",
            "The answer to that question was not found until the twentieth century; it\n",
            "required the development of a model for the atom. We therefore turn next to a\n",
            "closer examination of the atoms that make up all matter. M A K I N G  C O N N E\n",
            "C T I O N S The Rainbow Rainbows are an excellent illustration of the dispersion\n",
            "of sunlight. You have a good chance of seeing a rainbow any time you are between\n",
            "the Sun and a rain shower, as illustrated in Figure 5.13. The raindrops act like\n",
            "little prisms and break white light into the spectrum of colors. Suppose a ray\n",
            "of sunlight encounters a raindrop and passes i...\n",
            "\n",
            "[3]  Score = 0.4178\n",
            "------------------------------------------------------------\n",
            "In 1868, observations of the chromospheric spectrum revealed a yellow emission\n",
            "line that did not correspond to any previously known element on Earth.\n",
            "Scientists quickly realized they had found a new element and named it helium\n",
            "(after helios, the Greek word for ‚ÄúSun‚Äù). It took until 1895 for helium to be\n",
            "discovered on our planet. Today, students are probably most familiar with it as\n",
            "the light gas used to inflate balloons, although it turns out to be the second-\n",
            "most abundant element in the universe. The temperature of the chromosphere is\n",
            "about 10,000 K. This means that the chromosphere is hotter...\n",
            "\n",
            "[4]  Score = 0.3696\n",
            "------------------------------------------------------------\n",
            "At a still-higher setting, it glows a brighter orange-red (shorter wavelength).\n",
            "At even higher temperatures, which cannot be reached with ordinary stoves, metal\n",
            "can appear brilliant yellow or even blue-white. We can use these ideas to come\n",
            "up with a rough sort of ‚Äúthermometer‚Äù for measuring the temperatures of stars.\n",
            "Because many stars give off most of their energy in visible light, the color of\n",
            "light that dominates a star‚Äôs appearance is a rough indicator of its\n",
            "temperature. If one star looks red and another looks blue, which one 158 Chapter\n",
            "5 Radiation and Spectra This OpenStax book is avail...\n",
            "\n",
            "[5]  Score = 0.1848\n",
            "------------------------------------------------------------\n",
            "We have all seen an example of reddening on Earth. The Sun appears much redder\n",
            "at sunset than it does at noon. The lower the Sun is in the sky, the longer the\n",
            "path its light must travel through the atmosphere. Over this greater distance,\n",
            "there is a greater chance that sunlight will be scattered. Since red light is\n",
            "less likely to be scattered than blue light, the Sun appears more and more red\n",
            "as it approaches the horizon. By the way, scattering of sunlight is also what\n",
            "causes our sky to look blue, even though the gases that make up Earth‚Äôs\n",
            "atmosphere are transparent. As sunlight comes in, it sc...\n",
            "\n",
            "============================================================\n",
            "\n",
            "Ask a question (or type 'exit' to quit): How do astronomers infer the orbital motions of stars inside a galaxy?\n",
            "\n",
            "[RAGEngine] Query: How do astronomers infer the orbital motions of stars inside a galaxy?\n",
            "[RAGEngine] Retrieving chunks...\n",
            "[RAGEngine] Building prompt...\n",
            "[RAGEngine] Generating answer...\n",
            "\n",
            "============================================================\n",
            "üü¶ FINAL ANSWER\n",
            "============================================================\n",
            "How do astronomers infer the orbital motions of stars inside a galaxy?  The text\n",
            "describes two indirect techniques used to infer the orbital motions of stars\n",
            "inside a galaxy:  **1. Doppler Effect:** - This technique measures the change in\n",
            "the star‚Äôs radial velocity as the planet goes around the star. - The amount of\n",
            "wobble in the star‚Äôs motion determines the mass and orbital period of the\n",
            "planet.  **2. Transiting Planet:** - This technique detects the slight dimming\n",
            "of a star when one of its planets transits, or crosses over the face of the\n",
            "star. - The amount of starlight obscured can measure the planet‚Äôs size and\n",
            "orbital period.\n",
            "\n",
            "============================================================\n",
            "üü© TOP RETRIEVED CHUNKS\n",
            "============================================================\n",
            "\n",
            "[1]  Score = 0.6846\n",
            "------------------------------------------------------------\n",
            "the star to wobble, changing its radial velocity by a small but detectable\n",
            "amount. The distance of the star does not matter, as long as it is bright enough\n",
            "for us to take very high quality spectra. Measurements of the variation in the\n",
            "star‚Äôs radial velocity as the planet goes around the star can tell us the mass\n",
            "and orbital period of the planet. If there are several planets present, their\n",
            "effects on the radial velocity can be disentangled, so the entire planetary\n",
            "system can be deciphered‚Äîas long as the planets are massive enough to produce a\n",
            "measureable Doppler effect. This detection technique...\n",
            "\n",
            "[2]  Score = 0.6367\n",
            "------------------------------------------------------------\n",
            "From the observed motion and the period of the ‚Äúwiggle,‚Äù they could deduce the\n",
            "mass of Jupiter and its distance using Kepler‚Äôs laws. ( To refresh your memory\n",
            "about these laws, see the chapter on Orbits and Gravity.) Measuring positions in\n",
            "the sky this accurately is extremely difficult, and so far, astronomers have not\n",
            "made any confirmed detections of planets using this technique. However, we have\n",
            "been successful in using spectrometers to measure the changing velocity of stars\n",
            "with planets around them. As the star and planet orbit each other, part of their\n",
            "motion will be in our line of sight (t...\n",
            "\n",
            "[3]  Score = 0.4302\n",
            "------------------------------------------------------------\n",
            "As they consume their gas, the rate of star formation will slow down, and the\n",
            "spiral arms will gradually become less conspicuous. Over long periods, spirals\n",
            "therefore begin to look more like the galaxies at the middle of Figure 26.6\n",
            "(which astronomers refer to as S0 types). Over the past several decades, the\n",
            "study of how galaxies evolve over the lifetime of the universe has become one of\n",
            "the most active fields of astronomical research. We will discuss the evolution\n",
            "of galaxies in more detail in The Evolution and Distribution of Galaxies, but\n",
            "let‚Äôs first see in a little more detail just what di...\n",
            "\n",
            "[4]  Score = 0.3316\n",
            "------------------------------------------------------------\n",
            "Long ago, Newton showed that if you have matter distributed in the shape of a\n",
            "sphere, then it is simple to calculate the pull of gravity on some object just\n",
            "outside that sphere: you can assume that gravity acts as if all the matter were\n",
            "concentrated at a point in the center of the sphere. For our calculation, then,\n",
            "we can assume that all the mass that lies inward of the Sun‚Äôs position is\n",
            "concentrated at the center of the Galaxy, and that the Sun orbits that point\n",
            "from a distance of about 26,000 light-years. This is the sort of situation to\n",
            "which Kepler‚Äôs third law (as modified by Newton) can b...\n",
            "\n",
            "[5]  Score = 0.3213\n",
            "------------------------------------------------------------\n",
            "small separation and very hard to see at the distances of stars. This is why\n",
            "many of these systems are known to be double only through careful study of their\n",
            "spectra. We can analyze a radial velocity curve (such as the one in Figure 18.7)\n",
            "to determine the masses of the stars in a spectroscopic binary. This is complex\n",
            "in practice but not hard in principle. We measure the speeds of the stars from\n",
            "the Doppler effect. We then determine the period‚Äîhow long the stars take to go\n",
            "through an orbital cycle‚Äîfrom the velocity curve. Knowing how fast the stars are\n",
            "moving and how long they take to go around...\n",
            "\n",
            "============================================================\n",
            "\n",
            "Ask a question (or type 'exit' to quit): exit\n",
            "\n",
            "Exiting RAG system. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ceF1MnSieoLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}